[2025-04-09T20:57:37.263+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2025-04-09T20:57:36.614387+00:00 [queued]>
[2025-04-09T20:57:37.266+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2025-04-09T20:57:36.614387+00:00 [queued]>
[2025-04-09T20:57:37.266+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-04-09T20:57:37.270+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2025-04-09 20:57:36.614387+00:00
[2025-04-09T20:57:37.272+0000] {standard_task_runner.py:57} INFO - Started process 59 to run task
[2025-04-09T20:57:37.274+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2025-04-09T20:57:36.614387+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpifpgrpq4']
[2025-04-09T20:57:37.274+0000] {standard_task_runner.py:85} INFO - Job 7: Subtask reddit_extraction
[2025-04-09T20:57:37.294+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2025-04-09T20:57:36.614387+00:00 [running]> on host 7645e7e94702
[2025-04-09T20:57:37.323+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Hari Shankar' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2025-04-09T20:57:36.614387+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-09T20:57:36.614387+00:00'
[2025-04-09T20:57:37.324+0000] {logging_mixin.py:151} WARNING - Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.
[2025-04-09T20:57:37.325+0000] {logging_mixin.py:151} INFO - connected to reddit!
[2025-04-09T20:57:37.891+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I used to think great leadership meant knowing *everything* — every ticket, every schema change, every data quality issue, every pull request.\n\nYou know... "being a hands-on lead."\n\nBut here’s what my team’s messages were actually saying:\n\n“Hey, just checking—should this column be nullable or not?”  \n“Waiting on your review before I merge the dbt changes.”  \n“Can you confirm the DAG schedule again before I deploy?”\n\nThat’s when I realized: I wasn’t empowering my team — I was slowing them down.\n\nThey *could’ve* made those calls. But I’d unintentionally created a culture where they felt they needed my sign-off… even for small stuff.\n\nWhat hit me hardest,  wasn’t being helpful. I was micromanaging with extra steps.  \nAnd the more I inserted myself, the less confident the team became in their own decision-making.\n\nI’ve been working on backing off and designing better async systems — especially in how we surface blockers, align on schema changes, and handle github without turning it into “approval theater.”\n\n**Curious if other data/infra folks have been through this:**\n\n* How do you keep autonomy high *and* prevent chaos?\n* How do you create trust in decisions without needing to touch everything?\n\nWould love to learn from how others have handled this as your team grows.', 'author_fullname': 't2_sqp6mlirk', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I thought I was being a responsible tech lead… but I was just micromanaging in disguise', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv3rgi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 66, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 66, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744199118.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I used to think great leadership meant knowing <em>everything</em> — every ticket, every schema change, every data quality issue, every pull request.</p>\n\n<p>You know... &quot;being a hands-on lead.&quot;</p>\n\n<p>But here’s what my team’s messages were actually saying:</p>\n\n<p>“Hey, just checking—should this column be nullable or not?”<br/>\n“Waiting on your review before I merge the dbt changes.”<br/>\n“Can you confirm the DAG schedule again before I deploy?”</p>\n\n<p>That’s when I realized: I wasn’t empowering my team — I was slowing them down.</p>\n\n<p>They <em>could’ve</em> made those calls. But I’d unintentionally created a culture where they felt they needed my sign-off… even for small stuff.</p>\n\n<p>What hit me hardest,  wasn’t being helpful. I was micromanaging with extra steps.<br/>\nAnd the more I inserted myself, the less confident the team became in their own decision-making.</p>\n\n<p>I’ve been working on backing off and designing better async systems — especially in how we surface blockers, align on schema changes, and handle github without turning it into “approval theater.”</p>\n\n<p><strong>Curious if other data/infra folks have been through this:</strong></p>\n\n<ul>\n<li>How do you keep autonomy high <em>and</em> prevent chaos?</li>\n<li>How do you create trust in decisions without needing to touch everything?</li>\n</ul>\n\n<p>Would love to learn from how others have handled this as your team grows.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv3rgi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='IllWasabi8734'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv3rgi/i_thought_i_was_being_a_responsible_tech_lead_but/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv3rgi/i_thought_i_was_being_a_responsible_tech_lead_but/', 'subreddit_subscribers': 293349, 'created_utc': 1744199118.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.893+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am curious if there are any European analytics solutions as alternative to the large cloud providers and US giants like Databricks and Snowflake? Thinking about either query engines or lakehouse providers. Given the current political situation it seems like data sovereignty will be key in the future.', 'author_fullname': 't2_wlk3omh5b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there a European alternative to US analytical platforms like Snowflake?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv4gb5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 38, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 38, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744201376.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am curious if there are any European analytics solutions as alternative to the large cloud providers and US giants like Databricks and Snowflake? Thinking about either query engines or lakehouse providers. Given the current political situation it seems like data sovereignty will be key in the future.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv4gb5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='wenz0401'), 'discussion_type': None, 'num_comments': 43, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv4gb5/is_there_a_european_alternative_to_us_analytical/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv4gb5/is_there_a_european_alternative_to_us_analytical/', 'subreddit_subscribers': 293349, 'created_utc': 1744201376.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.893+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I didn't find much recent info on this cert when I was preparing for it, so I wanted to offer the opportunity for others to ask questions about content.\n\nTo be upfront I think that platform certifications like this are not a good measure of knowledge or ability and are a waste of time for most people in most situations. I'd only recommend pursuing this and similar exams if you have a specific requirement from an employer or client, or are applying to lots of roles where the certification is a hard requirement.\n\nFor a little background I work in consulting and have about 6 years of experience doing analytics and DE work in GCP in Senior level Analyst/DE Roles. I am a strong SQL practitioner and a passable Python programmer (no ML/AI stuff, just data plumbing).\n\nThe exam is 50 questions and it took me about 45 minutes. As with any GCP exam you don't get a percentage score or any details about what you got wrong/right, just a PASS or FAIL as soon as you hit submit on the final question. I have read varying theories on the passing score, most folks seem to think it is somewhere between 65% and 85%.\n\nThe questions were all multiple choice with a short paragraph giving a hypothetical situation and asking which GCP products should be deployed, or describing a problem with an existing deployment and asking how to troubleshoot. A few questions were 'select all that apply' style. There were some pretty basic tests of knowledge about Spark/Beam/Airflow specific programming concepts, and some very basic BigQuery SQL optimization stuff, but it was much more centered around provisioning, monitoring/troubleshooting, and selecting the right services based on volume, security, cost, and latency requirements.\n\nExam questions I got very roughly in order of prevalence were:\n\n* Dataproc and Dataflow\n* BigQuery and Cloud Storage\n* Data Lake (Dataplex, BQ Omni, etc)\n* Other Databases (Cloud SQL, Bigtable, Memorystore)\n* Networking\n* Dataform, Data Fusion.\n* Cloud Run\n* One very basic question about ML data prep\n\nI have mostly used BigQuery, Cloud Storage, Cloud Composer, Cloud Run/Cloud Functions,  Networking (VPCs, firewalls, NAT, etc), and a little Cloud SQL. I would have probably gotten most of the questions right on these services if I had not studied, but I was glad I brushed up on Networking as it is a broad subject and I had not used some of the services and features they asked about.\n\nI have never used Dataproc, /Dataflow, BQ Omni, Dataplex, Bigtable, Memorystore, Dataform or Data Fusion in actual production scenarios. They just aren't prevalent in the type of work I do and I would not have passed the exam if I hadn't studied about these beforehand.\n\nHappy to answer anything else!", 'author_fullname': 't2_1dg3g47vbs', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Just passed the GCP Professional Data Engineer cert, AMA', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jva1zx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 29, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1744216552.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744216167.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I didn&#39;t find much recent info on this cert when I was preparing for it, so I wanted to offer the opportunity for others to ask questions about content.</p>\n\n<p>To be upfront I think that platform certifications like this are not a good measure of knowledge or ability and are a waste of time for most people in most situations. I&#39;d only recommend pursuing this and similar exams if you have a specific requirement from an employer or client, or are applying to lots of roles where the certification is a hard requirement.</p>\n\n<p>For a little background I work in consulting and have about 6 years of experience doing analytics and DE work in GCP in Senior level Analyst/DE Roles. I am a strong SQL practitioner and a passable Python programmer (no ML/AI stuff, just data plumbing).</p>\n\n<p>The exam is 50 questions and it took me about 45 minutes. As with any GCP exam you don&#39;t get a percentage score or any details about what you got wrong/right, just a PASS or FAIL as soon as you hit submit on the final question. I have read varying theories on the passing score, most folks seem to think it is somewhere between 65% and 85%.</p>\n\n<p>The questions were all multiple choice with a short paragraph giving a hypothetical situation and asking which GCP products should be deployed, or describing a problem with an existing deployment and asking how to troubleshoot. A few questions were &#39;select all that apply&#39; style. There were some pretty basic tests of knowledge about Spark/Beam/Airflow specific programming concepts, and some very basic BigQuery SQL optimization stuff, but it was much more centered around provisioning, monitoring/troubleshooting, and selecting the right services based on volume, security, cost, and latency requirements.</p>\n\n<p>Exam questions I got very roughly in order of prevalence were:</p>\n\n<ul>\n<li>Dataproc and Dataflow</li>\n<li>BigQuery and Cloud Storage</li>\n<li>Data Lake (Dataplex, BQ Omni, etc)</li>\n<li>Other Databases (Cloud SQL, Bigtable, Memorystore)</li>\n<li>Networking</li>\n<li>Dataform, Data Fusion.</li>\n<li>Cloud Run</li>\n<li>One very basic question about ML data prep</li>\n</ul>\n\n<p>I have mostly used BigQuery, Cloud Storage, Cloud Composer, Cloud Run/Cloud Functions,  Networking (VPCs, firewalls, NAT, etc), and a little Cloud SQL. I would have probably gotten most of the questions right on these services if I had not studied, but I was glad I brushed up on Networking as it is a broad subject and I had not used some of the services and features they asked about.</p>\n\n<p>I have never used Dataproc, /Dataflow, BQ Omni, Dataplex, Bigtable, Memorystore, Dataform or Data Fusion in actual production scenarios. They just aren&#39;t prevalent in the type of work I do and I would not have passed the exam if I hadn&#39;t studied about these beforehand.</p>\n\n<p>Happy to answer anything else!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1jva1zx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SewBrew'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jva1zx/just_passed_the_gcp_professional_data_engineer/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jva1zx/just_passed_the_gcp_professional_data_engineer/', 'subreddit_subscribers': 293349, 'created_utc': 1744216167.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.894+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'As a data engineer, I really like the control and customization that Azure offers.\nAt the same time, I can see how Fabric is more business-friendly and leans toward a low/no-code experience.\n\nBut with all the content and comparisons floating around the internet, why is no one talking about how insanely expensive Fabric is?! Seriously—am I missing something here?\n\n\n-\n', 'author_fullname': 't2_581v3vhu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure vs Microsoft Fabric?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1juu00b', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 22, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 22, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744161651.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As a data engineer, I really like the control and customization that Azure offers.\nAt the same time, I can see how Fabric is more business-friendly and leans toward a low/no-code experience.</p>\n\n<p>But with all the content and comparisons floating around the internet, why is no one talking about how insanely expensive Fabric is?! Seriously—am I missing something here?</p>\n\n<h2></h2>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1juu00b', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Dharneeshkar'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1juu00b/azure_vs_microsoft_fabric/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1juu00b/azure_vs_microsoft_fabric/', 'subreddit_subscribers': 293349, 'created_utc': 1744161651.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.894+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Greetings, this is my first post here. I've been working in DE for the last 5 years now doing various things with Airflow and Dagster.  I have a question regarding design of data flow from APIs to our database.\n\nI am using Dagster/Python to perform the API pulls and loads into Snowflake.\n\nMy team lead insists that we load JSON data into our Snowflake RAW\\_DATA in the following way:\n\nID (should be a surrogate/non-native PK)  \nPAYLOAD (raw JSON payload, either as a VARCHAR or VARIANT type)  \nCREATED\\_DATE (timestamp this row was created in Snowflake)  \nUPDATE\\_DATE (timestamp this row was updated in Snowflake)\n\nFlattening of the payload then happens in SQL as a plain View, which we currently autogenerate using Python and manually edit and add to Snowflake.\n\nHe does not want us (DE team) to use DBT to do any transforming of RAW\\_DATA. DBT is only for the Data Analyst team to use for creating models.\n\nThe main advantage I see to this approach is flexibility if the JSON schema changes. You can freely append/drop/insert/reorder/rename columns. whereas a normal table you can only drop, append, and rename.\n\nOn the downside, it is slow and clunky to parse with SQL and access the data as a view. It just seems inefficient to have to recompute the view and parse all those JSON payloads whenever you want to access the table.  \n  \nI'd much rather do the flattening in Python, either manually or using dlt. Some JSON payloads I 'pre-flatten' in Python to make them easier to parse in SQL.\n\nIs there a better way, or is this how you all handle this as well?", 'author_fullname': 't2_pu2pg', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to handle loading JSON API data into database in pipelines', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jusby0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1744156860.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744156572.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Greetings, this is my first post here. I&#39;ve been working in DE for the last 5 years now doing various things with Airflow and Dagster.  I have a question regarding design of data flow from APIs to our database.</p>\n\n<p>I am using Dagster/Python to perform the API pulls and loads into Snowflake.</p>\n\n<p>My team lead insists that we load JSON data into our Snowflake RAW_DATA in the following way:</p>\n\n<p>ID (should be a surrogate/non-native PK)<br/>\nPAYLOAD (raw JSON payload, either as a VARCHAR or VARIANT type)<br/>\nCREATED_DATE (timestamp this row was created in Snowflake)<br/>\nUPDATE_DATE (timestamp this row was updated in Snowflake)</p>\n\n<p>Flattening of the payload then happens in SQL as a plain View, which we currently autogenerate using Python and manually edit and add to Snowflake.</p>\n\n<p>He does not want us (DE team) to use DBT to do any transforming of RAW_DATA. DBT is only for the Data Analyst team to use for creating models.</p>\n\n<p>The main advantage I see to this approach is flexibility if the JSON schema changes. You can freely append/drop/insert/reorder/rename columns. whereas a normal table you can only drop, append, and rename.</p>\n\n<p>On the downside, it is slow and clunky to parse with SQL and access the data as a view. It just seems inefficient to have to recompute the view and parse all those JSON payloads whenever you want to access the table.  </p>\n\n<p>I&#39;d much rather do the flattening in Python, either manually or using dlt. Some JSON payloads I &#39;pre-flatten&#39; in Python to make them easier to parse in SQL.</p>\n\n<p>Is there a better way, or is this how you all handle this as well?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jusby0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='fetus-flipper'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jusby0/best_way_to_handle_loading_json_api_data_into/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jusby0/best_way_to_handle_loading_json_api_data_into/', 'subreddit_subscribers': 293349, 'created_utc': 1744156572.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.894+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi.\nI am not sure if it's a rant post or reality check.\nI am working as Data Engineer and nearing couple of years of experience now.\n\nThroughout my career I never did the real data engineering or learned stuff what people posted on internet or linkedin.\n\nEverything I got was either pre built or it needed fixing. Like in my whole experience I never got the chance to write SQL in detail. Or even if I did I would have failed. I guess that is the reason I am still failing offers. \n\nI work in consultancy so the projects I got were mostly just mediocre at best. And it was just labour work with tight deadlines to either fix things or work on the same pattern someone built something. I always got overworked maybe because my communication sucked. And was too tired to learn anything after job.\n\nI never even saw a real data warehouse at work. \nI can still write Python code and write SQL queries but what you can call mediocre. If you told me write some complex pipeline or query I would probably fail.\n\nI am not sure how I even got this far. And I still think about removing some of my experience from cv to apply for junior data engineer roles and learn the way it's meant to be. I'm still afraid to apply for Senior roles because I don't think I'll even qualify as Senior, or they might laugh at me for things I should know but I don't. \n\nI once got rejected just because they said I overcomplicated stuff when the pipeline should have been short and simple. I still think I should have done it better if I was even slightly better at data engineering. \n\nI am just lost. Any help will be appreciated. \nThanks", 'author_fullname': 't2_voioqmae', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is this normal? Being mediocre', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1jved6i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 24, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 24, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744226633.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi.\nI am not sure if it&#39;s a rant post or reality check.\nI am working as Data Engineer and nearing couple of years of experience now.</p>\n\n<p>Throughout my career I never did the real data engineering or learned stuff what people posted on internet or linkedin.</p>\n\n<p>Everything I got was either pre built or it needed fixing. Like in my whole experience I never got the chance to write SQL in detail. Or even if I did I would have failed. I guess that is the reason I am still failing offers. </p>\n\n<p>I work in consultancy so the projects I got were mostly just mediocre at best. And it was just labour work with tight deadlines to either fix things or work on the same pattern someone built something. I always got overworked maybe because my communication sucked. And was too tired to learn anything after job.</p>\n\n<p>I never even saw a real data warehouse at work. \nI can still write Python code and write SQL queries but what you can call mediocre. If you told me write some complex pipeline or query I would probably fail.</p>\n\n<p>I am not sure how I even got this far. And I still think about removing some of my experience from cv to apply for junior data engineer roles and learn the way it&#39;s meant to be. I&#39;m still afraid to apply for Senior roles because I don&#39;t think I&#39;ll even qualify as Senior, or they might laugh at me for things I should know but I don&#39;t. </p>\n\n<p>I once got rejected just because they said I overcomplicated stuff when the pipeline should have been short and simple. I still think I should have done it better if I was even slightly better at data engineering. </p>\n\n<p>I am just lost. Any help will be appreciated. \nThanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jved6i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pixel_pirate1'), 'discussion_type': None, 'num_comments': 16, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jved6i/is_this_normal_being_mediocre/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jved6i/is_this_normal_being_mediocre/', 'subreddit_subscribers': 293349, 'created_utc': 1744226633.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.895+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi there :) would love to share my open source project -\xa0[CocoIndex](https://github.com/cocoindex-io/cocoindex), ETL with incremental processing.\n\nGithub:\xa0[https://github.com/cocoindex-io/cocoindex](https://github.com/cocoindex-io/cocoindex)\n\nKey features\n\n* support custom logic\n*  support process heavy transformations - e.g., embeddings, heavy fan-outs\n* support change data capture and realtime incremental processing on source data updates beyond time-series data.\n* written in Rust, SDK in python.\n\nWould love your feedback, thanks!', 'author_fullname': 't2_19zhptl2dm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Open source ETL with incremental processing', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1juz5vj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744179748.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi there :) would love to share my open source project -\xa0<a href="https://github.com/cocoindex-io/cocoindex">CocoIndex</a>, ETL with incremental processing.</p>\n\n<p>Github:\xa0<a href="https://github.com/cocoindex-io/cocoindex">https://github.com/cocoindex-io/cocoindex</a></p>\n\n<p>Key features</p>\n\n<ul>\n<li>support custom logic</li>\n<li> support process heavy transformations - e.g., embeddings, heavy fan-outs</li>\n<li>support change data capture and realtime incremental processing on source data updates beyond time-series data.</li>\n<li>written in Rust, SDK in python.</li>\n</ul>\n\n<p>Would love your feedback, thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?auto=webp&s=2815aee6c8fe1332522f9f13877f781d27f8b137', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?width=108&crop=smart&auto=webp&s=f147f7b8f50190af42f933f486416bf018de8198', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?width=216&crop=smart&auto=webp&s=f25dc4829c97c804ecf907d6d1de6fa63611bc8b', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?width=320&crop=smart&auto=webp&s=51be1ef99d6252e6b591502e3be17054a616e891', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?width=640&crop=smart&auto=webp&s=22e0e55ee73668b7d8080fd497007282da37fbf7', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?width=960&crop=smart&auto=webp&s=67d31f1de657b81cca76b8b06af8f6ce79c5b515', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/05p3Vc9IMwzzMy5-vvp_8ympKizqKoI5WoAmFvOpsyU.jpg?width=1080&crop=smart&auto=webp&s=2d4bcaccdc9a9292b4b157ad2d8e301db375bab5', 'width': 1080, 'height': 540}], 'variants': {}, 'id': '7bTHlpcIZ9vnPsvVI2K0hxSCKoN27U-RHXzLto1EHmo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1juz5vj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Whole-Assignment6240'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1juz5vj/open_source_etl_with_incremental_processing/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1juz5vj/open_source_etl_with_incremental_processing/', 'subreddit_subscribers': 293349, 'created_utc': 1744179748.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.895+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'What are the tools that can do stateful computations for streaming data ?  I know there are tools like flink, beam which can do stateful computation but are so heavy for my use case to setup the whole infrastructure. So is there are any other alternatives to them ? Heard about faust, so how is it? And any other tools if you know please recommend. ', 'author_fullname': 't2_aenh4mw5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Stateful Computation over Streaming Data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jux0zt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744171445.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>What are the tools that can do stateful computations for streaming data ?  I know there are tools like flink, beam which can do stateful computation but are so heavy for my use case to setup the whole infrastructure. So is there are any other alternatives to them ? Heard about faust, so how is it? And any other tools if you know please recommend. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jux0zt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Suspicious_Peanut282'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jux0zt/stateful_computation_over_streaming_data/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jux0zt/stateful_computation_over_streaming_data/', 'subreddit_subscribers': 293349, 'created_utc': 1744171445.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.895+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My company decided to use AWS Batch exclusively for batch jobs, and we run everything on Fargate.  For dbt jobs, Batch works fine, but I haven't hit a use case where I use any Batch-specific features.  That is, I could just as well be using anything that can launch containers.\n\nI'm using dbt for loading a traditional Data Warehouse with sources that are updated daily or hourly, and jobs that run for a couple minutes.   Seems like batch adds features more relevant to machine learning workflows?   Like having intelligent/tunable prioritization of many instances of a few images.\n\nDoes anyone here make use of cool batch features relevant to loading DW from periodic vendor files?  Am I missing out?", 'author_fullname': 't2_abfpw4qq7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Running DBT core jobs on AWS with fargate -- Batch vs ECS', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv2id3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.74, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744194570.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My company decided to use AWS Batch exclusively for batch jobs, and we run everything on Fargate.  For dbt jobs, Batch works fine, but I haven&#39;t hit a use case where I use any Batch-specific features.  That is, I could just as well be using anything that can launch containers.</p>\n\n<p>I&#39;m using dbt for loading a traditional Data Warehouse with sources that are updated daily or hourly, and jobs that run for a couple minutes.   Seems like batch adds features more relevant to machine learning workflows?   Like having intelligent/tunable prioritization of many instances of a few images.</p>\n\n<p>Does anyone here make use of cool batch features relevant to loading DW from periodic vendor files?  Am I missing out?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv2id3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='levintennine'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv2id3/running_dbt_core_jobs_on_aws_with_fargate_batch/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv2id3/running_dbt_core_jobs_on_aws_with_fargate_batch/', 'subreddit_subscribers': 293349, 'created_utc': 1744194570.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.896+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone,\n\nI\'m in the early stages of setting up a **greenfield data platform** and would love to hear your insights.\n\nI’m planning to use **dbt** as the transformation layer, and as I research orchestration tools, **Dagster keeps coming up as the "go-to" if you\'re starting from scratch**. That said, one thing I keep running into: people talk about "Dagster" like it\'s one thing, but **rarely clarify if they mean the Community or Enterprise version**.\n\nFor those of you who’ve actually **self-hosted the Community version**—what\'s your experience been like?\n\n* Are there **key limitations** or features you ended up missing?\n* Did you start with Community and **later migrate to Enterprise**? If so, how smooth (or painful) was that?\n* What did you **wish you knew** before picking an orchestrator?\n\nI\'m pretty new to data platform architecture, and I’m hoping this thread can help others in the same boat. I’d really appreciate any practical advice or war stories from people who\'ve **been through the build-from-scratch journey**.\n\nAlso, if you’ve evaluated alternatives and *still* picked Dagster, I’d love to hear why. What really mattered as your project scaled?\n\nThanks in advance — happy to share back what I learn as I go!', 'author_fullname': 't2_14wthz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dagster Community vs Enterprise?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv1dkw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744189778.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone,</p>\n\n<p>I&#39;m in the early stages of setting up a <strong>greenfield data platform</strong> and would love to hear your insights.</p>\n\n<p>I’m planning to use <strong>dbt</strong> as the transformation layer, and as I research orchestration tools, <strong>Dagster keeps coming up as the &quot;go-to&quot; if you&#39;re starting from scratch</strong>. That said, one thing I keep running into: people talk about &quot;Dagster&quot; like it&#39;s one thing, but <strong>rarely clarify if they mean the Community or Enterprise version</strong>.</p>\n\n<p>For those of you who’ve actually <strong>self-hosted the Community version</strong>—what&#39;s your experience been like?</p>\n\n<ul>\n<li>Are there <strong>key limitations</strong> or features you ended up missing?</li>\n<li>Did you start with Community and <strong>later migrate to Enterprise</strong>? If so, how smooth (or painful) was that?</li>\n<li>What did you <strong>wish you knew</strong> before picking an orchestrator?</li>\n</ul>\n\n<p>I&#39;m pretty new to data platform architecture, and I’m hoping this thread can help others in the same boat. I’d really appreciate any practical advice or war stories from people who&#39;ve <strong>been through the build-from-scratch journey</strong>.</p>\n\n<p>Also, if you’ve evaluated alternatives and <em>still</em> picked Dagster, I’d love to hear why. What really mattered as your project scaled?</p>\n\n<p>Thanks in advance — happy to share back what I learn as I go!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv1dkw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Jobdriaan'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv1dkw/dagster_community_vs_enterprise/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv1dkw/dagster_community_vs_enterprise/', 'subreddit_subscribers': 293349, 'created_utc': 1744189778.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.896+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I used the command line to monitor the health of my data pipelines by reading logs to debug performance issues across my stack. But to be honest? The experience left a lot to be desired.\n\nBetween the poor ui and the flood of logs, **I found myself spending way too much time trying to trace what actually went wrong in a given run.**\n\nSo I built a tool that layers on top of any stack and uses retrieval augmented generation (I’m a data scientist by trade) to pull logs, system metrics, and anomalies together into plain-English **summaries of what happened, why and how to fix it.**\n\nAfter several iterations, **it’s helped me cut my debugging time by 10x.** No more sifting through dashboards or correlating logs across tools for hours.\n\nI’m [open-sourcing ](https://github.com/dingus-technology/CHAT-WITH-LOGS)it so others can benefit and [built a product version for hardcore users with advanced features](https://www.dingusai.dev).\n\nIf you’ve felt the pain of tracking down issues across fragmented sources, I’d love your thoughts. Could this help in your setup? Do you deal with the same kind of debugging mess?\n\n\\---\n\n[Example usage of k8 pods with issues and getting an resolution without viewing the logs](https://preview.redd.it/3xyo5r62erte1.png?width=1080&format=png&auto=webp&s=b52d5bba9b489305b543f49cd94b18a2a10db623)', 'author_fullname': 't2_8faanui6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I built a tool to outsource log tracing and debug my errors (it was overwhelming me so i fixed it)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'3xyo5r62erte1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 73, 'x': 108, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=108&crop=smart&auto=webp&s=2b56cfa273533817a81b9e06a157076bf3239db3'}, {'y': 147, 'x': 216, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=216&crop=smart&auto=webp&s=a99e48b0cbd7885709ae9eb2a0b8eff73984da86'}, {'y': 217, 'x': 320, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=320&crop=smart&auto=webp&s=90f8b0017ec49143da4fade37e7d21236e0d3561'}, {'y': 435, 'x': 640, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=640&crop=smart&auto=webp&s=a894501d98e0ad229c180f870a700075de7186d8'}, {'y': 653, 'x': 960, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=960&crop=smart&auto=webp&s=ee3f7c4df606717a5ba87aac72fac54363ffda85'}, {'y': 735, 'x': 1080, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=1080&crop=smart&auto=webp&s=3a5e4fb146097cf765328c93ec5d9b0eebcd82f6'}], 's': {'y': 735, 'x': 1080, 'u': 'https://preview.redd.it/3xyo5r62erte1.png?width=1080&format=png&auto=webp&s=b52d5bba9b489305b543f49cd94b18a2a10db623'}, 'id': '3xyo5r62erte1'}}, 'name': 't3_1juzwlp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/5mQl0DWClOfk5mKRNEVcX8G7DVcUJAQipk9i8fWyLXA.jpg', 'edited': 1744184484.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'subreddit_type': 'public', 'created': 1744183053.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I used the command line to monitor the health of my data pipelines by reading logs to debug performance issues across my stack. But to be honest? The experience left a lot to be desired.</p>\n\n<p>Between the poor ui and the flood of logs, <strong>I found myself spending way too much time trying to trace what actually went wrong in a given run.</strong></p>\n\n<p>So I built a tool that layers on top of any stack and uses retrieval augmented generation (I’m a data scientist by trade) to pull logs, system metrics, and anomalies together into plain-English <strong>summaries of what happened, why and how to fix it.</strong></p>\n\n<p>After several iterations, <strong>it’s helped me cut my debugging time by 10x.</strong> No more sifting through dashboards or correlating logs across tools for hours.</p>\n\n<p>I’m <a href="https://github.com/dingus-technology/CHAT-WITH-LOGS">open-sourcing </a>it so others can benefit and <a href="https://www.dingusai.dev">built a product version for hardcore users with advanced features</a>.</p>\n\n<p>If you’ve felt the pain of tracking down issues across fragmented sources, I’d love your thoughts. Could this help in your setup? Do you deal with the same kind of debugging mess?</p>\n\n<p>---</p>\n\n<p><a href="https://preview.redd.it/3xyo5r62erte1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b52d5bba9b489305b543f49cd94b18a2a10db623">Example usage of k8 pods with issues and getting an resolution without viewing the logs</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?auto=webp&s=383d21c0c758f395cdbd941ffc82b27a60c8d1d1', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?width=108&crop=smart&auto=webp&s=7c6a5c639c3fac3c014c86c64d8a7c4339edaaf5', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?width=216&crop=smart&auto=webp&s=a3cadb1b34b1a8bf8cbd985e61330c61e136e559', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?width=320&crop=smart&auto=webp&s=5b07453b5b216bf19a26d8e19dc9407f74541e99', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?width=640&crop=smart&auto=webp&s=f08afc59294e89eb7991d018c0beb6543d60d03d', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?width=960&crop=smart&auto=webp&s=5a1e0a84ef8b927d33367adf73cd7e8392c611d0', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/3wdkZIDnlfNOwAlXN4tuAUz1OPpqGqkHd7vRxkHcBuE.jpg?width=1080&crop=smart&auto=webp&s=e7da383e868e0e54b6d323365eab990f735c65f3', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Z_ymfa9VZklrSUARcw2G7CzrWUcD_pYmDRLE_b0oR5Y'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1juzwlp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SnooMuffins6022'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1juzwlp/i_built_a_tool_to_outsource_log_tracing_and_debug/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1juzwlp/i_built_a_tool_to_outsource_log_tracing_and_debug/', 'subreddit_subscribers': 293349, 'created_utc': 1744183053.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.897+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am trying to simplify and optimize an incrementally loading model in Dataform.\n\nCurrently I reload all source data partitions in the update window (7 days), which seems unnecessary.\n\nI was thinking about using the `INFORMATION_SCHEMA.PARTITIONS`\xa0view to determine which source partitions have been updated since the last run of the model.  **My question.... what is the best technique to find the last run timestamp of a Dataform model?**\n\nMy ideas:\n\n1. Go the dbt freshness route and add an `updated_at` timestamp column to each row in the model.  Then find the `MAX` of that in the last 7 days (or just be a little sloppy at get timestamp from newest partition and be OK with unnecessarily reloading a partition now and then.)\n2. Create a new table that is a **transaction log of the model runs**.  Log a start and end timestamp in there and use that very small table to get a last run timestamp.\n3. Look at `INFORMATION_SCHEMA.PARTITIONS` on the incremental model (not the source).  Use the `MAX` of that to determine the last time it was run.  I'm worried this could be updated in other ways and cause us to skip source data.\n4. Dig it out of `INFORMATION_SCHEMA.JOBS`.  Though I'm not sure it would contain what I need.\n5. Keep loading 7 days on each run but throttle it with a freshness check so it only happens X times per X.\n\nThanks!", 'author_fullname': 't2_4minn2zw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dataform incremental loads and last run timestamp', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv55e6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1744204021.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744203513.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am trying to simplify and optimize an incrementally loading model in Dataform.</p>\n\n<p>Currently I reload all source data partitions in the update window (7 days), which seems unnecessary.</p>\n\n<p>I was thinking about using the <code>INFORMATION_SCHEMA.PARTITIONS</code>\xa0view to determine which source partitions have been updated since the last run of the model.  <strong>My question.... what is the best technique to find the last run timestamp of a Dataform model?</strong></p>\n\n<p>My ideas:</p>\n\n<ol>\n<li>Go the dbt freshness route and add an <code>updated_at</code> timestamp column to each row in the model.  Then find the <code>MAX</code> of that in the last 7 days (or just be a little sloppy at get timestamp from newest partition and be OK with unnecessarily reloading a partition now and then.)</li>\n<li>Create a new table that is a <strong>transaction log of the model runs</strong>.  Log a start and end timestamp in there and use that very small table to get a last run timestamp.</li>\n<li>Look at <code>INFORMATION_SCHEMA.PARTITIONS</code> on the incremental model (not the source).  Use the <code>MAX</code> of that to determine the last time it was run.  I&#39;m worried this could be updated in other ways and cause us to skip source data.</li>\n<li>Dig it out of <code>INFORMATION_SCHEMA.JOBS</code>.  Though I&#39;m not sure it would contain what I need.</li>\n<li>Keep loading 7 days on each run but throttle it with a freshness check so it only happens X times per X.</li>\n</ol>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jv55e6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Seldon_Seen'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv55e6/dataform_incremental_loads_and_last_run_timestamp/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv55e6/dataform_incremental_loads_and_last_run_timestamp/', 'subreddit_subscribers': 293349, 'created_utc': 1744203513.0, 'num_crossposts': 2, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.897+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'If anyone on this sub has worked for Amazon as a Data engineer, preferably entry level or early careers, how has your experience been working at amazon at Amazon? \n\nI’ve heard their work culture is very startup like, and their is an abundance of poor managers. The company just cars about share holder value, instead of caring for their customers and employees.\n\nI wanted to hear on this sub, how has your experience been? How was the hiring process like? What all skills I should develop to work for Amazon?', 'author_fullname': 't2_16jvm6gcb3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How are entry level data engineering roles at Amazon?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1juo1uo', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.56, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744145011.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>If anyone on this sub has worked for Amazon as a Data engineer, preferably entry level or early careers, how has your experience been working at amazon at Amazon? </p>\n\n<p>I’ve heard their work culture is very startup like, and their is an abundance of poor managers. The company just cars about share holder value, instead of caring for their customers and employees.</p>\n\n<p>I wanted to hear on this sub, how has your experience been? How was the hiring process like? What all skills I should develop to work for Amazon?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1juo1uo', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='gta35'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1juo1uo/how_are_entry_level_data_engineering_roles_at/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1juo1uo/how_are_entry_level_data_engineering_roles_at/', 'subreddit_subscribers': 293349, 'created_utc': 1744145011.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.897+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've just published a blog post exploring how to orchestrate Dagster workflows using MCP:\xa0  \n[https://kyrylai.com/2025/04/09/dagster-llm-orchestration-mcp-server/](https://kyrylai.com/2025/04/09/dagster-llm-orchestration-mcp-server/)  \n  \nAlso included a straightforward implementation of a Dagster MCP server with OpenAI’s Agent SDK. Appreciate any feedback!", 'author_fullname': 't2_16jbmvoh8r', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Orchestrate Your Data via LLMs: Meet the Dagster MCP Server', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jvdbiz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744224081.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve just published a blog post exploring how to orchestrate Dagster workflows using MCP:\xa0<br/>\n<a href="https://kyrylai.com/2025/04/09/dagster-llm-orchestration-mcp-server/">https://kyrylai.com/2025/04/09/dagster-llm-orchestration-mcp-server/</a>  </p>\n\n<p>Also included a straightforward implementation of a Dagster MCP server with OpenAI’s Agent SDK. Appreciate any feedback!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?auto=webp&s=90c9ff63abab6c4f9d48286beaf19e6021c2ac55', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?width=108&crop=smart&auto=webp&s=5b57efd6dc4660342f28d636ace9656e6491b855', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?width=216&crop=smart&auto=webp&s=c2ddee7b72feadf9fbf5ad42343a84698ff9507c', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?width=320&crop=smart&auto=webp&s=44f4d933b40fd9a7b107b4d821457986bc2ac50d', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?width=640&crop=smart&auto=webp&s=57d9fdd65d2a8830d0ec6f7fcea33311ccf293e6', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?width=960&crop=smart&auto=webp&s=2c94755f25451c64add19353b6f5315adc23cd00', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/UDaFCJUYBzatEwiEiMoy9i7HB_7Q0GyY3bNoSMeJIxQ.jpg?width=1080&crop=smart&auto=webp&s=89b48d529762d314912153ea7da68fc1a004de61', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'advQCY7oxpEPX4OwN-VOGgjRbqpC4RawppUCvgMIEMc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1jvdbiz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Such_Tale_9830'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jvdbiz/orchestrate_your_data_via_llms_meet_the_dagster/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jvdbiz/orchestrate_your_data_via_llms_meet_the_dagster/', 'subreddit_subscribers': 293349, 'created_utc': 1744224081.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.898+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "As my first task in my first data engineering role, I am doing a trade study looking at on-premises storage solutions. \n\nOur use case involves diverse data types (timeseries, audio, video, SW logs, and more) in the neighborhood of thousands of terabytes to dozens of petabytes. The end use-case is analytics and development of ML models. \n\n\\***disclaimer**: I'm a data scientist with no real experience as a data engineer, so please forgive and kindly correct any nonsense that I say. \n\nBased on my research so far, it appears that you can get away with a single technology for storing all types of data, i.e. \n\n* force a traditional relational database to serve you image data along side structured data,\n*  or throw structured data in an S3 bucket or MinIO along side images. \n\nThis might reduce cost/complexity/setup time on a new project being run by a noob like me, but reduce efficiency. On the other hand, it seems like it might be better to tailor a suite of solutions like a combination of: \n\n* MinIO or HDFS (audio/video) \n* ClickHouse or TimescaleDB (sensor timeseries data) \n* Postgres (the relational bits, like system user data)\n\nThe draw back here is that each of these technologies has their own learning curve, and might be difficult for a noob like me to set up, leading to having to hire more folks. But, maybe that's worth it. \n\nYour inputs are very much appreciated. Let me know if I can answer any questions that might help you help me!", 'author_fullname': 't2_zk44m', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Single technology storage solution or specialized suite?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jvawq7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744218257.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As my first task in my first data engineering role, I am doing a trade study looking at on-premises storage solutions. </p>\n\n<p>Our use case involves diverse data types (timeseries, audio, video, SW logs, and more) in the neighborhood of thousands of terabytes to dozens of petabytes. The end use-case is analytics and development of ML models. </p>\n\n<p>*<strong>disclaimer</strong>: I&#39;m a data scientist with no real experience as a data engineer, so please forgive and kindly correct any nonsense that I say. </p>\n\n<p>Based on my research so far, it appears that you can get away with a single technology for storing all types of data, i.e. </p>\n\n<ul>\n<li>force a traditional relational database to serve you image data along side structured data,</li>\n<li> or throw structured data in an S3 bucket or MinIO along side images. </li>\n</ul>\n\n<p>This might reduce cost/complexity/setup time on a new project being run by a noob like me, but reduce efficiency. On the other hand, it seems like it might be better to tailor a suite of solutions like a combination of: </p>\n\n<ul>\n<li>MinIO or HDFS (audio/video) </li>\n<li>ClickHouse or TimescaleDB (sensor timeseries data) </li>\n<li>Postgres (the relational bits, like system user data)</li>\n</ul>\n\n<p>The draw back here is that each of these technologies has their own learning curve, and might be difficult for a noob like me to set up, leading to having to hire more folks. But, maybe that&#39;s worth it. </p>\n\n<p>Your inputs are very much appreciated. Let me know if I can answer any questions that might help you help me!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jvawq7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='wcneill'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jvawq7/single_technology_storage_solution_or_specialized/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jvawq7/single_technology_storage_solution_or_specialized/', 'subreddit_subscribers': 293349, 'created_utc': 1744218257.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.898+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I’m about to start a data engineering internship and I’m currently studying Business Analytics (Focus on application of ML Models) and I’ve already done \\~1 year of internship experience in data engineering, mostly working on ETL pipelines and some ML framework coding.\n\nImportant context: I don’t learn coding in school, so I’ve been self-taught so far.\n\nI want to sharpen my skills and make the best use of my time before the internship kicks off. Should I go for:\n\n* Harvard’s **CS50: Introduction to Computer Science** ([https://pll.harvard.edu/course/cs50-introduction-computer-science](https://pll.harvard.edu/course/cs50-introduction-computer-science)), or\n* This **Comprehensive Python Course** on YouTube ([https://www.youtube.com/watch?v=XKHEtdqhLK8](https://www.youtube.com/watch?v=XKHEtdqhLK8))?\n\nI’m torn between building stronger CS fundamentals vs. focusing on  Python skills. Which would be more beneficial at this point?', 'author_fullname': 't2_5a8zb1la', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'CS50 or Full Python Course', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv7wf6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744210869.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I’m about to start a data engineering internship and I’m currently studying Business Analytics (Focus on application of ML Models) and I’ve already done ~1 year of internship experience in data engineering, mostly working on ETL pipelines and some ML framework coding.</p>\n\n<p>Important context: I don’t learn coding in school, so I’ve been self-taught so far.</p>\n\n<p>I want to sharpen my skills and make the best use of my time before the internship kicks off. Should I go for:</p>\n\n<ul>\n<li>Harvard’s <strong>CS50: Introduction to Computer Science</strong> (<a href="https://pll.harvard.edu/course/cs50-introduction-computer-science">https://pll.harvard.edu/course/cs50-introduction-computer-science</a>), or</li>\n<li>This <strong>Comprehensive Python Course</strong> on YouTube (<a href="https://www.youtube.com/watch?v=XKHEtdqhLK8">https://www.youtube.com/watch?v=XKHEtdqhLK8</a>)?</li>\n</ul>\n\n<p>I’m torn between building stronger CS fundamentals vs. focusing on  Python skills. Which would be more beneficial at this point?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/voKNcoQcWTArrF0nnx6_l5J17wKsyiKfORiJ13aEmMo.jpg?auto=webp&s=88e9d90de8afca42bf770261563a7be7e18313cd', 'width': 530, 'height': 298}, 'resolutions': [{'url': 'https://external-preview.redd.it/voKNcoQcWTArrF0nnx6_l5J17wKsyiKfORiJ13aEmMo.jpg?width=108&crop=smart&auto=webp&s=f3ca773e79ab413ea2144a4adfab3137cf51d16a', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/voKNcoQcWTArrF0nnx6_l5J17wKsyiKfORiJ13aEmMo.jpg?width=216&crop=smart&auto=webp&s=26ea9a738296960e7a0ed72eefdfb0143847f5d0', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/voKNcoQcWTArrF0nnx6_l5J17wKsyiKfORiJ13aEmMo.jpg?width=320&crop=smart&auto=webp&s=76b9ffa96e051b8df49a11e77031e1dd0924807b', 'width': 320, 'height': 179}], 'variants': {}, 'id': 'a7RB1ke08nlTvmJsUnFQhfOoACXPjH4TOQSN3Ct4Hjg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1jv7wf6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ticklemydizzle'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv7wf6/cs50_or_full_python_course/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv7wf6/cs50_or_full_python_course/', 'subreddit_subscribers': 293349, 'created_utc': 1744210869.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.898+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '\n\nI feel like it has been same thing these past 8 years but the competition is still quite high in this field, some tell you have to find a niche but does it niche really work in this field?\n\nI have been off my career for 5 month now and still haven’t figured out what to do, I really want continue and develop a unique or offering solution for companies.\nI’m a BI engineer and mostly using Microsoft products. \n\nAny advice?', 'author_fullname': 't2_enuh99is', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Overwhelmed and not feeling what to do next to develop a unique skills set', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv4xvy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744202868.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I feel like it has been same thing these past 8 years but the competition is still quite high in this field, some tell you have to find a niche but does it niche really work in this field?</p>\n\n<p>I have been off my career for 5 month now and still haven’t figured out what to do, I really want continue and develop a unique or offering solution for companies.\nI’m a BI engineer and mostly using Microsoft products. </p>\n\n<p>Any advice?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1jv4xvy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='maximazie'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv4xvy/overwhelmed_and_not_feeling_what_to_do_next_to/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv4xvy/overwhelmed_and_not_feeling_what_to_do_next_to/', 'subreddit_subscribers': 293349, 'created_utc': 1744202868.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.898+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi,\n\nI was wondering if some of you, or your company as a whole, came up with an idea, of how to force users to import only quality data into the system (like ERP). It does not have to be perfect, but some schema enforcement etc.\n\nDid you find any solution to this, is it a problem at all for you?', 'author_fullname': 't2_1g1w2bnbst', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Forcing users to keep data clean', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv1gua', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744190196.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi,</p>\n\n<p>I was wondering if some of you, or your company as a whole, came up with an idea, of how to force users to import only quality data into the system (like ERP). It does not have to be perfect, but some schema enforcement etc.</p>\n\n<p>Did you find any solution to this, is it a problem at all for you?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jv1gua', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MedicalBodybuilder49'), 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv1gua/forcing_users_to_keep_data_clean/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv1gua/forcing_users_to_keep_data_clean/', 'subreddit_subscribers': 293349, 'created_utc': 1744190196.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.899+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_975og', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Snowflake Data Lineage Guide: From Metadata to Data Governance', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 38, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv1bh3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/LOg4ewj9avoCuj-lsvXdfXOLZgzzcmNT21QMAMUwF9c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1744189506.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'selectstar.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.selectstar.com/resources/snowflake-data-lineage', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?auto=webp&s=83332b231c2a9bf438f2c8d6e3696a98c748c65d', 'width': 1600, 'height': 445}, 'resolutions': [{'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?width=108&crop=smart&auto=webp&s=9a7fd58978f70b3c826073a66b3dee7d9299cd43', 'width': 108, 'height': 30}, {'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?width=216&crop=smart&auto=webp&s=a6667dc33b47f22a34eb21e4f941fecf1e87f942', 'width': 216, 'height': 60}, {'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?width=320&crop=smart&auto=webp&s=71949fc2eb66e95204d586cfa96f11d3db65bf24', 'width': 320, 'height': 89}, {'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?width=640&crop=smart&auto=webp&s=800ddaefe1f85424035fd500189920f546f5621b', 'width': 640, 'height': 178}, {'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?width=960&crop=smart&auto=webp&s=8673a9d9881c6d831c01dc78e29ad755a81b7743', 'width': 960, 'height': 267}, {'url': 'https://external-preview.redd.it/KnIS7LTXT3Ces-OSZxM7V7Mre-f_Gueui3Yl_uWiAh4.jpg?width=1080&crop=smart&auto=webp&s=5b43cd412c2762c61409a1d95964af64ca35fa81', 'width': 1080, 'height': 300}], 'variants': {}, 'id': 'Zo7zr19XdeuS6RKpZgHvheyIlLZ4jr9a3LJcxtBb8_s'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1jv1bh3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='arimbr'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv1bh3/snowflake_data_lineage_guide_from_metadata_to/', 'stickied': False, 'url': 'https://www.selectstar.com/resources/snowflake-data-lineage', 'subreddit_subscribers': 293349, 'created_utc': 1744189506.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.899+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Do anyone have a good approach to discover changes in the data for records with nested structures (containing arrays), preferably with Spark?\n\nI have not found any good solution to this. On approach could be to md5 a json-object of the record, but arrays would have to be sorted to only check for changes in the data, and not ordering of sub records in arrays. \n\n\n', 'author_fullname': 't2_fzlphdfm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best approach to check for changes in records with nested structures', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1juzfe3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744180920.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Do anyone have a good approach to discover changes in the data for records with nested structures (containing arrays), preferably with Spark?</p>\n\n<p>I have not found any good solution to this. On approach could be to md5 a json-object of the record, but arrays would have to be sorted to only check for changes in the data, and not ordering of sub records in arrays. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1juzfe3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Practical-Swim-999'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1juzfe3/best_approach_to_check_for_changes_in_records/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1juzfe3/best_approach_to_check_for_changes_in_records/', 'subreddit_subscribers': 293349, 'created_utc': 1744180920.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.899+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have had conversations with quite a few data engineers recently. About 80% of them don't know what it takes to go to the next level. To be fair, I didn't have a formal matrix until a couple of years too.\n\nNow, the actual job matrix is only for paid subscribers, but you really don't need it. I've posted the complete guide as well as the AI prompt for completely free.\n\nAnyways, do you have a career progression framework at your org? I'd love to swap notes!", 'author_fullname': 't2_1c6f704', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Made a job ladder that doesn’t suck. Sharing my thought process in case your team needs one.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1jvesrw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': 'transparent', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/v2bqhoXeJqgPaN5QNuDKAiqS1kERrBnVfSV8cFmec5g.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1744227734.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'datagibberish.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have had conversations with quite a few data engineers recently. About 80% of them don&#39;t know what it takes to go to the next level. To be fair, I didn&#39;t have a formal matrix until a couple of years too.</p>\n\n<p>Now, the actual job matrix is only for paid subscribers, but you really don&#39;t need it. I&#39;ve posted the complete guide as well as the AI prompt for completely free.</p>\n\n<p>Anyways, do you have a career progression framework at your org? I&#39;d love to swap notes!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://datagibberish.com/p/building-a-techincal-job-level-matrix', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?auto=webp&s=c32c8364dd48d6a76956aa84d3392256680ebee8', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?width=108&crop=smart&auto=webp&s=076c2e6bcefb1ef1533561cfbc864f5f975dc6f5', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?width=216&crop=smart&auto=webp&s=db84f4d9b32cf059f71de4d5e06e10b8a51582bf', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?width=320&crop=smart&auto=webp&s=15a7cac7b82ed3d95ebf7248ae5af0b62418db82', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?width=640&crop=smart&auto=webp&s=f9aa1e1c697ce49d97e720740c6d150c550eea80', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?width=960&crop=smart&auto=webp&s=b2d8e7df1a3ee5e085dcd0cc3e384a0f999f6a73', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/meTWBAZHnj4Kl2lG7s5TrUAnhr41QvJU7qt_SrXGNE0.jpg?width=1080&crop=smart&auto=webp&s=eb552038895f3c6d63dd5dc79cacda106b0192dd', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'qQPB9A3LZG_VrFNY52g8jBWzphtJwBFPatsIo5JA8mM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1jvesrw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ivanovyordan'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1jvesrw/made_a_job_ladder_that_doesnt_suck_sharing_my/', 'stickied': False, 'url': 'https://datagibberish.com/p/building-a-techincal-job-level-matrix', 'subreddit_subscribers': 293349, 'created_utc': 1744227734.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.900+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am loading data from SQL DB to Azure storage account and will be using change data capture resource in Azure Data Factory to incrementally process data. Question is how do I go about loading in the historical data as CDC will only process the changes. There are changes being implemented on the SQL DB table all the time. If I do a copy activity to load in all the historical data, and I already have CDC enabled on my source table.\n\nWould CDC resource duplicate what is already there in my historical load? How do I ensure that I don't duplicate/miss any transactions? I have looked at all the documentation (I think) surrounding this, but the answer is not clear on the specifics of my question.", 'author_fullname': 't2_i1jyh2yq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Change Data Capture Resource ADF', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv63we', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744206199.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am loading data from SQL DB to Azure storage account and will be using change data capture resource in Azure Data Factory to incrementally process data. Question is how do I go about loading in the historical data as CDC will only process the changes. There are changes being implemented on the SQL DB table all the time. If I do a copy activity to load in all the historical data, and I already have CDC enabled on my source table.</p>\n\n<p>Would CDC resource duplicate what is already there in my historical load? How do I ensure that I don&#39;t duplicate/miss any transactions? I have looked at all the documentation (I think) surrounding this, but the answer is not clear on the specifics of my question.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jv63we', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Evening-Ad-8479'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv63we/change_data_capture_resource_adf/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv63we/change_data_capture_resource_adf/', 'subreddit_subscribers': 293349, 'created_utc': 1744206199.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.900+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My goal is to re create something like Oracle's Net-suite, are there any help full resources on how i can go about it. i have previously worked on simple Finance management systems but this one is more complicated. i need sample ERD's books or anything helpfull atp\n\n", 'author_fullname': 't2_10u24v78z2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Designing a database ERP from scratch.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jun8gx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744142965.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My goal is to re create something like Oracle&#39;s Net-suite, are there any help full resources on how i can go about it. i have previously worked on simple Finance management systems but this one is more complicated. i need sample ERD&#39;s books or anything helpfull atp</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1jun8gx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Specific_Bad8942'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jun8gx/designing_a_database_erp_from_scratch/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jun8gx/designing_a_database_erp_from_scratch/', 'subreddit_subscribers': 293349, 'created_utc': 1744142965.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.900+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello. We have a lot of Bigquery SQL models, but there are two specific models (the number won't grow much in the future), that will be much better done in python. We have some microservices that could do that in a later stage of the pipeline, and it's fine.\n\nFor coherence, it would be nice though to have them as python models. So how is Dataproc to work with? How is your experience with the setup? We will use the serverless option because we won't be using the cluster for anything else. Is it very easy to setup or in the other hand is not worth the added complexity?\n\nThanks!", 'author_fullname': 't2_chl6zxlwv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dbt python models on BigQuery. Is Dataproc nice to work with?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv25fe', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744193110.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello. We have a lot of Bigquery SQL models, but there are two specific models (the number won&#39;t grow much in the future), that will be much better done in python. We have some microservices that could do that in a later stage of the pipeline, and it&#39;s fine.</p>\n\n<p>For coherence, it would be nice though to have them as python models. So how is Dataproc to work with? How is your experience with the setup? We will use the serverless option because we won&#39;t be using the cluster for anything else. Is it very easy to setup or in the other hand is not worth the added complexity?</p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv25fe', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JLTDE'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv25fe/dbt_python_models_on_bigquery_is_dataproc_nice_to/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv25fe/dbt_python_models_on_bigquery_is_dataproc_nice_to/', 'subreddit_subscribers': 293349, 'created_utc': 1744193110.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.900+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "We are considering moving away from Pentaho to Abinitio. I am supposed to reasearch on why abinitio could be better choice. Fyi : organisation is heavily dependent on abinitio and pentaho supports just one part , we are considering moving that to Abinitio.\n\nIt's would be really greate if anyone who worked on both could provide some insights.", 'author_fullname': 't2_vn1meuc3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Pentaho vs Abinitio', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv1q4z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744191335.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We are considering moving away from Pentaho to Abinitio. I am supposed to reasearch on why abinitio could be better choice. Fyi : organisation is heavily dependent on abinitio and pentaho supports just one part , we are considering moving that to Abinitio.</p>\n\n<p>It&#39;s would be really greate if anyone who worked on both could provide some insights.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jv1q4z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='jojobaoil68'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv1q4z/pentaho_vs_abinitio/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv1q4z/pentaho_vs_abinitio/', 'subreddit_subscribers': 293349, 'created_utc': 1744191335.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.901+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Im leading my first data engineering project with basically non existent experience (transactional background). Very lost on how to architect the project.\n\nWe have some data in azure in a ADLS gen 2 in delta format, with a star schema structure. The goal is to perform analytics on it from a rest microservice to display charts in a customer frontend.\n\nRight now, the idea is from a spring microservice make queries through synapse, but the cost is very high. I'm sure this is something that other people must be doing more efficiently... what is the best approach?\n\nSchedule a spark job in databricks/airflow to dump aggregates in a sql table? Read the delta directly in Java?\n\nI would love to hear your opinions", 'author_fullname': 't2_19aq1u', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'REST interface to consume delta lake analytics', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv1h9h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744190251.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Im leading my first data engineering project with basically non existent experience (transactional background). Very lost on how to architect the project.</p>\n\n<p>We have some data in azure in a ADLS gen 2 in delta format, with a star schema structure. The goal is to perform analytics on it from a rest microservice to display charts in a customer frontend.</p>\n\n<p>Right now, the idea is from a spring microservice make queries through synapse, but the cost is very high. I&#39;m sure this is something that other people must be doing more efficiently... what is the best approach?</p>\n\n<p>Schedule a spark job in databricks/airflow to dump aggregates in a sql table? Read the delta directly in Java?</p>\n\n<p>I would love to hear your opinions</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jv1h9h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='TheOneAndOnlyFrog'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv1h9h/rest_interface_to_consume_delta_lake_analytics/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv1h9h/rest_interface_to_consume_delta_lake_analytics/', 'subreddit_subscribers': 293349, 'created_utc': 1744190251.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.901+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "So I have a table that basically calculates 2 measures and these 2 measures rules change by financial year. \n\nWhat I envision is this table will be as so. The natural primary key columns + financial year as the primary key.\n\nSo the table would look something like below for example. Basically the same record gets loaded more than once with different years\n\npk1 pk2 financialYear  KPI\n1.    1.     22/23.              29\n1.    1.     23/24.              32\n\n\nWhat would be the best way to load this type of table using purely SQL and stored procedure? \n\nMy first idea is just having multiple insert statements but I can foresee the code getting bigger as the years pass.\n\nI will probably add that I'm on SQL Server only and it's only moving data from one table to another.\n\nThanks!\n", 'author_fullname': 't2_2zzjrjlz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Loading data that falls within multiple years', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv0s82', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1744196811.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744187061.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>So I have a table that basically calculates 2 measures and these 2 measures rules change by financial year. </p>\n\n<p>What I envision is this table will be as so. The natural primary key columns + financial year as the primary key.</p>\n\n<p>So the table would look something like below for example. Basically the same record gets loaded more than once with different years</p>\n\n<p>pk1 pk2 financialYear  KPI\n1.    1.     22/23.              29\n1.    1.     23/24.              32</p>\n\n<p>What would be the best way to load this type of table using purely SQL and stored procedure? </p>\n\n<p>My first idea is just having multiple insert statements but I can foresee the code getting bigger as the years pass.</p>\n\n<p>I will probably add that I&#39;m on SQL Server only and it&#39;s only moving data from one table to another.</p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv0s82', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='flaglord21'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv0s82/loading_data_that_falls_within_multiple_years/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv0s82/loading_data_that_falls_within_multiple_years/', 'subreddit_subscribers': 293349, 'created_utc': 1744187061.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.901+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour\n\n[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)', 'author_fullname': 't2_3p620rl9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1juvgjf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.53, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744166216.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><h1>FREE Azure Course for Beginners | Learn Azure &amp; Data Bricks in 1 Hour</h1>\n\n<p><a href="https://www.youtube.com/watch?v=8XH2vTyzL7c">https://www.youtube.com/watch?v=8XH2vTyzL7c</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/TjEA42HbXYWyYpdAQZyeDWA7P0rhGyn9AOl_Lmxhi1M.jpg?auto=webp&s=72285c9e9d54cf251e30179b05063b73166c470b', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/TjEA42HbXYWyYpdAQZyeDWA7P0rhGyn9AOl_Lmxhi1M.jpg?width=108&crop=smart&auto=webp&s=65276c288ea4a2855d184c4c2538a06584307c79', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/TjEA42HbXYWyYpdAQZyeDWA7P0rhGyn9AOl_Lmxhi1M.jpg?width=216&crop=smart&auto=webp&s=76e004ba606c366bd35f60e5925bdd7e458c219a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/TjEA42HbXYWyYpdAQZyeDWA7P0rhGyn9AOl_Lmxhi1M.jpg?width=320&crop=smart&auto=webp&s=ece6efcd159a60138fe8ac21e37d539ff3225ea9', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'p_0YM79l9SqZqSOv01NdAkFBH5s-0_Xd_LzsBpN79Ig'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'CEO of Data Engineer Academy', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1juvgjf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='chrisgarzon19'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1juvgjf/azure_course_for_beginners_learn_azure_data/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1juvgjf/azure_course_for_beginners_learn_azure_data/', 'subreddit_subscribers': 293349, 'created_utc': 1744166216.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.901+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am having not great luck in finding a job In my field even though I have 6yoe. I'm currently studying my masters to try and stay in the game -- but since I'm unemployed is there any other work that I could put my skills to? Most places for hourly won't hire me because I'm over qualified. So I've been doing Uber. But is there any other stuff I could do? Freelance work? Low level? I'm also new to this country so not super sure what my options are. ", 'author_fullname': 't2_1i8khmxapr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Other work for Data Engineers?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv8o0t', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744212751.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am having not great luck in finding a job In my field even though I have 6yoe. I&#39;m currently studying my masters to try and stay in the game -- but since I&#39;m unemployed is there any other work that I could put my skills to? Most places for hourly won&#39;t hire me because I&#39;m over qualified. So I&#39;ve been doing Uber. But is there any other stuff I could do? Freelance work? Low level? I&#39;m also new to this country so not super sure what my options are. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1jv8o0t', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Acceptable_Oil900'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv8o0t/other_work_for_data_engineers/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv8o0t/other_work_for_data_engineers/', 'subreddit_subscribers': 293349, 'created_utc': 1744212751.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.902+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey folks,\n\nJust stumbled upon an upcoming webinar that looks interesting, especially if you’re into data observability, lineage, and quality frameworks. It’s hosted by Rakuten SixthSense and seems to focus on best practices for managing large-scale data pipelines and ensuring reliability across the stack.\n\nMight be useful if you’re dealing with:\n\n\nData drift or broken pipelines\n\nETL/ELT monitoring across tools\n\nLack of visibility into your data\n\nhttps://www.linkedin.com/posts/rakuten-sixthsense_dataobservability-dataquality-webinar-activity-7315252322320691200-ia-J?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAEc2p7MBZSL7xm2f3KOIsdrMp0ThEcJ3TDc\n\n\nWould love to know if anyone here has used Rakuten’s data tools or attended their sessions before. Are they worth tuning in for?\n\nNot affiliated – just sharing in case it helps someone.', 'author_fullname': 't2_13p1hqmqzw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Free Webinar on Modern Data Observability & Quality – Worth Checking Out?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1jv53ne', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1744213938.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1744203364.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey folks,</p>\n\n<p>Just stumbled upon an upcoming webinar that looks interesting, especially if you’re into data observability, lineage, and quality frameworks. It’s hosted by Rakuten SixthSense and seems to focus on best practices for managing large-scale data pipelines and ensuring reliability across the stack.</p>\n\n<p>Might be useful if you’re dealing with:</p>\n\n<p>Data drift or broken pipelines</p>\n\n<p>ETL/ELT monitoring across tools</p>\n\n<p>Lack of visibility into your data</p>\n\n<p><a href="https://www.linkedin.com/posts/rakuten-sixthsense_dataobservability-dataquality-webinar-activity-7315252322320691200-ia-J?utm_source=social_share_send&amp;utm_medium=member_desktop_web&amp;rcm=ACoAAEc2p7MBZSL7xm2f3KOIsdrMp0ThEcJ3TDc">https://www.linkedin.com/posts/rakuten-sixthsense_dataobservability-dataquality-webinar-activity-7315252322320691200-ia-J?utm_source=social_share_send&amp;utm_medium=member_desktop_web&amp;rcm=ACoAAEc2p7MBZSL7xm2f3KOIsdrMp0ThEcJ3TDc</a></p>\n\n<p>Would love to know if anyone here has used Rakuten’s data tools or attended their sessions before. Are they worth tuning in for?</p>\n\n<p>Not affiliated – just sharing in case it helps someone.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?auto=webp&s=bd52dd2b25b95fc9c2a47a54b4496d00449e8516', 'width': 1400, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?width=108&crop=smart&auto=webp&s=f9bafd7ce5cdc8400b3dec1967a6f34a28315c6b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?width=216&crop=smart&auto=webp&s=fdf27a59b4f1377d92d6ffc035a5a9a74860e9c5', 'width': 216, 'height': 123}, {'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?width=320&crop=smart&auto=webp&s=f5c488120e6ee70f930944ee03722a4cb45bdda5', 'width': 320, 'height': 182}, {'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?width=640&crop=smart&auto=webp&s=7099dea11134e671bd9975452cc1271bb7e1853d', 'width': 640, 'height': 365}, {'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?width=960&crop=smart&auto=webp&s=c4c3bfc50fb21f6f2ad1a35df568717e78a9094e', 'width': 960, 'height': 548}, {'url': 'https://external-preview.redd.it/uDsZzeCMBZ8kk48S8s7dHHriA6QG6DmnLKcj2rFLdkQ.jpg?width=1080&crop=smart&auto=webp&s=7e63879e78dbc8104aced8dcd62b1ba9cf52317f', 'width': 1080, 'height': 617}], 'variants': {}, 'id': 'CjbMbFq2MEqSKWpNCjv-ipCLADmRBQ1ZCG3w2yy71f0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1jv53ne', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Adventurous_Okra_846'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1jv53ne/free_webinar_on_modern_data_observability_quality/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1jv53ne/free_webinar_on_modern_data_observability_quality/', 'subreddit_subscribers': 293349, 'created_utc': 1744203364.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.902+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff8979ef70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.\n\n\nI’ve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017–2024. \n\nI trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups — including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) — as inputs to predict game-level performance in 2024.\n\nThe model performs really well for some stats (e.g., R² > 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others — like Touchdowns, Fumbles, or Yards per Target — aren’t as strong.\n\nHere’s where I need input:\n\n-What’s a solid baseline R², RMSE, and MAE to aim for — and does that benchmark shift depending on the industry?\n\n-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-R² ones, something else for low-R²)?\n\n-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?\n\n-I used XGBRegressor based on common recommendations — are there variants of XGBoost or alternatives you'd suggest trying? Any others you like better?\n\n-Are these considered “good” model results for sports data?\n\n-Are sports models generally harder to predict than industries like retail, finance, or real estate?\n\n-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?\n\n-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more player’s coming back from injury, etc.? Is this a generally accepted method or not really utilized?\n\nAny advice, criticism, resources, or just general direction is welcomed.", 'author_fullname': 't2_38onlj83', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'is_gallery': True, 'title': 'Beginner Predictive Model Feedback/Guidance', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 62, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'yn52ffg10qte1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 47, 'x': 108, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=108&crop=smart&auto=webp&s=2a159f14c65d8921af77d03d10197b19cb920a44'}, {'y': 95, 'x': 216, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=216&crop=smart&auto=webp&s=15e6c5bf15db7a842ecf6d90edc8f86c6bd66816'}, {'y': 142, 'x': 320, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=320&crop=smart&auto=webp&s=33316cdf8adc66ec0ba0f9117004f6d18bd4d2a0'}, {'y': 284, 'x': 640, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=640&crop=smart&auto=webp&s=6a3f6270f973785abf7b0f965edd13606d3b72fd'}, {'y': 426, 'x': 960, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=960&crop=smart&auto=webp&s=12337e76143acc6c1f7b3d6d59a42ed00d8fa14f'}, {'y': 479, 'x': 1080, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=1080&crop=smart&auto=webp&s=f382cc8a6b285fb91c69210c3f7b2a3873bdff4c'}], 's': {'y': 535, 'x': 1204, 'u': 'https://preview.redd.it/yn52ffg10qte1.jpg?width=1204&format=pjpg&auto=webp&s=4e3a231cb9ef7a2bcc95920f0b20f169098addb0'}, 'id': 'yn52ffg10qte1'}, 'zv0m2gg10qte1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 47, 'x': 108, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=108&crop=smart&auto=webp&s=ae38fe2b8df20c0e633adb392d5cac578f739c39'}, {'y': 94, 'x': 216, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=216&crop=smart&auto=webp&s=2318cb72d4127f867311fd2cb882d6415c22a39e'}, {'y': 139, 'x': 320, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=320&crop=smart&auto=webp&s=11366ce8be46cfffe2e22a745f5b067db26f6fc7'}, {'y': 279, 'x': 640, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=640&crop=smart&auto=webp&s=bd93a1780ba8ff304e5adf5299ed19b7d202143d'}, {'y': 419, 'x': 960, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=960&crop=smart&auto=webp&s=bcf0b05c31227c3557832f83b33be084259b89f6'}, {'y': 471, 'x': 1080, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=1080&crop=smart&auto=webp&s=f3278d861be69bc652218f568f3eb8bff7389008'}], 's': {'y': 526, 'x': 1204, 'u': 'https://preview.redd.it/zv0m2gg10qte1.jpg?width=1204&format=pjpg&auto=webp&s=488487fe18bf2e80acf8636bc3b77118428b7d74'}, 'id': 'zv0m2gg10qte1'}, '9nbm9fg10qte1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 49, 'x': 108, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=108&crop=smart&auto=webp&s=eb7aa5d88622a609a139b6f559cf61ffaf4e7408'}, {'y': 99, 'x': 216, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=216&crop=smart&auto=webp&s=1050fe294fdfea8b18e0a52df18cdaf2b4f2fd95'}, {'y': 146, 'x': 320, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=320&crop=smart&auto=webp&s=b09a0f3e608a3dc46304f998e2aa86ce0dd13ed6'}, {'y': 293, 'x': 640, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=640&crop=smart&auto=webp&s=0f38c0ae418005e70601a2c46719c7b3e86d92ed'}, {'y': 440, 'x': 960, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=960&crop=smart&auto=webp&s=0ff9865f11309c45de56f549236419be8c0d0a24'}, {'y': 496, 'x': 1080, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=1080&crop=smart&auto=webp&s=2f8ca3f5b4996e6525d59c864d979d4994ecfb35'}], 's': {'y': 553, 'x': 1204, 'u': 'https://preview.redd.it/9nbm9fg10qte1.jpg?width=1204&format=pjpg&auto=webp&s=50c8df649ea27c665ee6fd6a6f19aa8206a14024'}, 'id': '9nbm9fg10qte1'}, 'dyloigg10qte1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 86, 'x': 108, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=108&crop=smart&auto=webp&s=e2e293cad44ed36d566f39cf66b0bf91032104c5'}, {'y': 172, 'x': 216, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=216&crop=smart&auto=webp&s=961b7c2a7ff4b7a20a8b82dd8437920f82db6ec5'}, {'y': 255, 'x': 320, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=320&crop=smart&auto=webp&s=957e6e26f0c4dcdb30a2880b798a17dfcdd8942f'}, {'y': 511, 'x': 640, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=640&crop=smart&auto=webp&s=341d1969ebe13c120d0a3418b3cad30f4eb2b3dc'}, {'y': 767, 'x': 960, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=960&crop=smart&auto=webp&s=8b18cf7f7b6e39ee206772649df816b80ab4b2ce'}, {'y': 862, 'x': 1080, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=1080&crop=smart&auto=webp&s=d7b9f6751863a952897dd9cb68ff7127749b18c1'}], 's': {'y': 962, 'x': 1204, 'u': 'https://preview.redd.it/dyloigg10qte1.jpg?width=1204&format=pjpg&auto=webp&s=b391377c4225f9e64d7fcfffd8b53c21b6ffdbb2'}, 'id': 'dyloigg10qte1'}}, 'name': 't3_1juvakz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'ups': 0, 'domain': 'reddit.com', 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'gallery_data': {'items': [{'media_id': 'yn52ffg10qte1', 'id': 639568242}, {'media_id': 'zv0m2gg10qte1', 'id': 639568243}, {'media_id': '9nbm9fg10qte1', 'id': 639568244}, {'media_id': 'dyloigg10qte1', 'id': 639568245}]}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/wxF8D3LfD0N-BKZoOusDy3bgjCtc6k_TjTVnGlDZyzU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1744165677.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'total_awards_received': 0, 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.</p>\n\n<p>I’ve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017–2024. </p>\n\n<p>I trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups — including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) — as inputs to predict game-level performance in 2024.</p>\n\n<p>The model performs really well for some stats (e.g., R² &gt; 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others — like Touchdowns, Fumbles, or Yards per Target — aren’t as strong.</p>\n\n<p>Here’s where I need input:</p>\n\n<p>-What’s a solid baseline R², RMSE, and MAE to aim for — and does that benchmark shift depending on the industry?</p>\n\n<p>-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-R² ones, something else for low-R²)?</p>\n\n<p>-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?</p>\n\n<p>-I used XGBRegressor based on common recommendations — are there variants of XGBoost or alternatives you&#39;d suggest trying? Any others you like better?</p>\n\n<p>-Are these considered “good” model results for sports data?</p>\n\n<p>-Are sports models generally harder to predict than industries like retail, finance, or real estate?</p>\n\n<p>-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?</p>\n\n<p>-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more player’s coming back from injury, etc.? Is this a generally accepted method or not really utilized?</p>\n\n<p>Any advice, criticism, resources, or just general direction is welcomed.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/1juvakz', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1juvakz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ynwFreddyKrueger'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1juvakz/beginner_predictive_model_feedbackguidance/', 'stickied': False, 'url': 'https://www.reddit.com/gallery/1juvakz', 'subreddit_subscribers': 293349, 'created_utc': 1744165677.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2025-04-09T20:57:37.902+0000] {python.py:194} INFO - Done. Returned value was: None
[2025-04-09T20:57:37.910+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20250409T205736, start_date=20250409T205737, end_date=20250409T205737
[2025-04-09T20:57:37.933+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-04-09T20:57:37.941+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
